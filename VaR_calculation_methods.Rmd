---
title: "Ilościowe miary ryzyka rynkowego - projekt zaliczeniowy"
author: "Aleksandra Talaga"
date: "2025-01-28"
output: 
  html_document:
    code_folding: show
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r, echo=FALSE}
knitr::opts_chunk$set(
  warning = FALSE,  
  message = FALSE)
```

<br>

```{r}
# Potrzebne pakiety
library(ggplot2)
library(dplyr)
library(gridExtra)
library(moments)
library(nortest)
library(knitr)
library(rugarch)
library(tidyr)

set.seed(123)
```

<br>

# 1. Wstępny opis danych

<br>
Dane wykorzystane do projektu dotyczą kursu waluty euro względem polskiego złotego. Są to dane dzienne, z okresu między 2007 a 2024 rokiem. Celem projektu jest zbadanie stóp zwrotu, a także wyznaczenie różnymi metodami Value at Risk, próbując przy tym odpowiedzieć na pytanie, która z metod sprawdziła się najlepiej.

```{r}
euro <- read.csv("C:/Studia/Semestr 5/Ilosciowe miary ryzyka rynkowego/GIELDA/eurpln_d.csv")
euro$Data <- as.Date(euro$Data)

euro <- euro %>%
  mutate(Log_stopa = log(Zamkniecie/lag(Zamkniecie)))

euro <- euro[-1,]
```

<br>

Dane zawierają kolumny: <br>
- Data <br>
- Otwarcie (kurs waluty przy otwarciu giełdy) <br>
- Najwyższy (najwyższy kurs waluty z danego dnia) <br>
- Najniższy (najniższy kurs waluty z danego dnia) <br>
- Zamknięcie (kurs waluty przy zamknięciu giełdy) <br><br>
Dodatkowo zostały obliczone logarytmiczne stopy zwrotu. Informują one jak zmieniała się wartość (w tym wypadku waluty) względem dnia poprzedniego. Kluczowe w projekcie będą przede wszystkim wartości stóp zwrotu oraz wartości kursu waluty przy zamknięciu giełdy. Pozostałe zmienne nie zostaną wykorzystane.

```{r}
kable(head(euro, 5))
```


<br>

## 1.1. Wykresy danych w czasie

<br>
```{r}
e1 <- euro %>%
  ggplot() +
  geom_line(aes(x = Data, y = Zamkniecie), color = "#3366cc") +
  labs(
    title = "Kurs EUR-PLN (2007-2024)", y = "Kurs") +
  scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "1 year"),
    date_labels = "%Y") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


e2 <- euro %>%
  ggplot() +
  geom_line(aes(x = Data, y = Log_stopa), color = "#3366cc") +
  labs(
    title = "Wykres stóp zwrotu (2007-2024)") +
  scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "1 year"),
    date_labels = "%Y") +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(e1, e2, ncol = 1)
```

<br>
Wykresy prezentują zmianę kursu EUR-PLN oraz stóp zwrotu w czasie od 2007 roku do końca roku 2024. Początkowo kurs euro malał, aż do roku 2009, kiedy to nastąpiło gwałtowne ożywienie i w krótkim czasie kurs wzrósł od minimalnej wartości około 3.25 aż do niemal 5. Można zauważyć, że właśnie wtedy stopy zwrotu najbardziej się wahały i osiągały największe wartości. Przez następne lata kurs euro stabilizował się. Wynosił wtedy około 4.25zł i były momenty, kiedy delikatnie rósł (do 4.50), a następnie spadał, ale jednak był dość stabilny przez około 10 lat. Od roku 2020, a więc od początku pandemii coronavirusa, widać rosnący trend - euro znów drożało. W szczycie około 2022 roku znów osiągnęło cenę niemal 5zł. Był to czas wysokiej ifnlacji w Polsce, która wynosiła wówczas kilkanaście procent, co świadczyło o słabej sile złotego. Zbiegło się to także z wybuchem wojny w Ukrainie, co również mogło powodować osłabienie polskiej waluty (i zapewne nie tylko polskiej, ale także walut innych krajów środkowo-wschodniej Europy). Z nadejściem 2023 roku, kiedy ustabilizowała się zarówno iflacja w Polsce, jak i sytuacja dotycząca wojny w Ukrainie (świat się przyzwyczaił do prowadzonej tam wojny), cena euro względem złotego znów zaczęła spadać, a następnie się stabilizować. Pod koniec ubiegłego roku za 1 euro trzeba było zapłacić około 4.30zł. Warto zauważyć, że wtedy, gdy kurs był stabilny, małe były stopy zwrotu. Natomiast kiedy kurs nagle spadał lub nagle wzrastał, stopy zwrotu były duże. Wynika to wprost ze sposobu obliczania stóp zwrotu jako ilorazu kursu dnia dzisiejszego z dniem poprzednim.


<br>

## 1.2. Statystyki opisowe

<br>

```{r}
statystyki_opisowe <- data.frame(matrix(ncol = 7, nrow = 2))


oblicz_statystyki <- function(kolumna)
{
  return(
    c(
      round(mean(kolumna),2),
      round(sd(kolumna),2),
      paste0(round(sd(kolumna) / mean(kolumna) * 100, 2), "%"),
      round(min(kolumna),2),
      round(max(kolumna),2),
      round(skewness(kolumna),2),
      round(kurtosis(kolumna),2)
    )
  )
}


statystyki_opisowe[,] <- t(sapply(euro[, c(5,6)], oblicz_statystyki))

rownames(statystyki_opisowe) <- colnames(euro[, c(5,6)])
colnames(statystyki_opisowe) <-  c("Srednia", "Odchylenie", "Wsp. zmiennosci", "Min", "Max", "Skosnosc", "Kurtoza")

kable(statystyki_opisowe)
```

**Średni kurs** euro na przestrzeni lat wynosił około 4.23zł i odchylał się od niej przeciętnie o 23 grosze. Względnie wynosi to niecałe 7% zmian. Oznacza to, że kurs euro względem złotego jest dośc stabilny. Wartości minimalne i maksymalne pokazują jednak, że kurs ten miał także momenty większych wahań. Minimalna wartość to 3.20zł (około roku 2009), a maksymalna to niemal 5zł (na początku 2022 roku). Rozkład jest lewostronnie skośny, oznacza to, że występują bardzo małe wartości (w niewielkiej ilości), które "ciągną" cały rozkład w lewo. Kurtoza wskazuje na leptokurtyczność, czyli wartości są porozrzucane blisko wokół średniej. <br><br>
**Stopy zwrotu** mają średnią w przybliżeniu 0 oraz odchylenie standardowe wynoszące 0.01. Współczynnik zmienności jest ogromny, wynika to z faktu, że dzielimy odchylenie standardowe przez średnią (która jest bliska zeru). Zarówno największe straty jak i największe zyski z dnia na dzień wynosiły 4%. Rozkład stóp zwrotu jest niemal symetryczny (minimalna asymetria prawostronna, wskazująca, że występują pewne duże wartości na prawo od średniej). Kurtoza jest bardzo wysoka, czyli stopy zwrotu często przyjmują wartości blisko średniej (blisko 0 ).

<br><br>

## 1.3. Testy normalności

<br>

```{r}
testy_normalnosci <- data.frame(
  Shapiro = shapiro.test(euro$Log_stopa)$p.value,
  Lillieforsa  = lillie.test(euro$Log_stopa)$p.value,
  Andersona = ad.test(euro$Log_stopa)$p.value
)

testy_normalnosci <- round(testy_normalnosci, 3)
rownames(testy_normalnosci) <- "P-value"

kable(testy_normalnosci)
```

<br>
W zaokrągleniu do 3 miejsc po przecinku wszystkie p-value wynoszą 0. Oznacza to, że wszystkie tetsy odrzucają hipotezę zerową mówiącą o tym, że stopy zwrotu opisane są rozkładem normalnym. Jest tak dlatego, że do badania wzięto dane dzienne. Pomimo, że często przyjmuje się założenie o normalności stóp zwrotu, dopiero dla danych miesięcznych jest duża szansa, że faktycznie rozkład będzie normalny.

## 1.4. Rozkłady 

<br>

**Rozkład stóp zwrotu**

```{r}
m = mean(euro$Log_stopa)
s = sd(euro$Log_stopa)

euro %>%
  ggplot() +
  geom_density(aes(x = Log_stopa), fill = "#3366cc", alpha = 0.3) +
  stat_function(fun = dnorm, args = list(mean = m, sd = s), color = "red", linewidth = 1) +
  labs(
    title = "Teoretyczny i empiryczny rozkład stóp zwrotu") +
  theme_bw() 
```
Na rysunku przedstawiono empiryczny rozkład stóp zwrotu (kolor niebieski) oraz teoretyczny rozkład normalny o średniej 0 oraz odchyleniu standardowym 0.01. Dobrze widać, dlaczego hipoteza o normalności została odrzucona. Rozkład empiryczny ma o wiele więcej wartości bliskich 0. Jest bardzo dużo niewielkich stóp zwrotu lub takich, które różnią się niewiele od średniej. Są to momenty wtedy, kiedy kurs nie zmienia się gwałtownie, ale zachowuje stabilność. Jest mało "średnich" odchyleń od średniej. Rozkład empiryczny charakteryzuje się natomiast grubymi ogonami i tym, że o wiele częściej niż w teorii zdarzają się bardzo duże odchylenia od średniej. Reguła 3 sigm mówi o tym, że jedynie około 0.27% obserwacji będzie znajdować się dalej niż 3 odchylenia standardowe od średniej, a obserwacje znajdujące się dalej o kilka odchyleń standardowych powinny być tak rzadkie, że nigdy nie zaobserwowalibyśmy ich w ciągu naszego życia. Jednak w praktyce takie obserwacje bardzo daleko odstające od średniej zdarzają się o wiele częściej. Wciąż jednak są to rzadkie sytuacje, ale nie niemożliwe do zaobserwowania.

```{r}
teoretycznie = c(
  pnorm(m + s, mean = m, sd = s) - pnorm(m - s, mean = m, sd = s),
  (pnorm(m + 2*s, mean = m, sd = s) - pnorm(m + s, mean = m, sd = s)) * 2,
  (pnorm(m + 3* s, mean = m, sd = s) - pnorm(m + 2* s, mean = m, sd = s)) * 2,
  (pnorm(m + 4 *s, mean = m, sd = s) - pnorm(m + 3* s, mean = m, sd = s)) * 2,
  (1 - pnorm(m + 4*s, mean = m, sd = s))* 2
)
Log_stopa <- euro$Log_stopa

empirycznie = c( sum(Log_stopa > m - s & Log_stopa < m + s) / length(Log_stopa),
                 ((sum(Log_stopa > m + s & Log_stopa < m + 2*s) + sum(Log_stopa > m -2*s & Log_stopa < m -s)))  / length(Log_stopa),
                 ((sum(Log_stopa > m + 2*s & Log_stopa < m + 3*s) + sum(Log_stopa > m -3*s & Log_stopa < m -2*s)))  / length(Log_stopa),
                 ((sum(Log_stopa > m + 3*s & Log_stopa < m + 4*s) + sum(Log_stopa > m -4*s & Log_stopa < m -3*s)))  / length(Log_stopa),
                 sum(Log_stopa > m + 4*s | Log_stopa < m - 4*s) / length(Log_stopa)
)

odchylenia <- data.frame(
  SD = c("< 1SD", "1-2 SD", "2-3 SD", "3-4 SD", "> 5SD"),
  Teoretycznie = paste(round(teoretycznie,4) * 100, "%"),
  Empirycznie = paste(round(empirycznie,4) * 100, "%")
)
kable(odchylenia)
```

Tabela przedstawia, jaki odsetek obserwacji powinien w teorii przekraczać średnią o ustaloną liczbę odchyleń standardowych. Druga kolumna obrazuje, jak wygląda to w praktyce. O ponad 10 punktów procetowych więcej jest danych, które w praktyce nie przekraczają 1 odchylenia standardowego niż wskazuje na to teoria. Mniej jest natomiast "średnich" przekroczeń czyli o 2 lub 3 odchylenia standardowe. Różnice o 3-4 odchylenia standardowe zdarzają się w praktyce ponad 4 razy częściej, natomiast różnice o 5 odchyleń standardowych już 76 razy częściej. W praktyce odchylenie o 5sd powinno pojawiać się co 10000 obserwacji czyli co 27 lat, a empirycznie zdarza się co około 130 obserwacji.



<br><br>

**Rozkład wartości kursu**

```{r}
euro %>%
  ggplot() +
  geom_density(aes(x = Zamkniecie), color = "#3366cc") +
  labs(
    title = "Rozkład kursu EUR-PLN") +
  theme_bw() 
```
<br>
Jeśli chodzi o tego, jak wygląda rozkład kursu w czasie zamknięcia giełdy, wskazuje to powyższy wykres. Rozkład jest dość nieregularny. Widać lewostronną skośność. 

<br><br>

# 2. Funkcje liczące VaR

<br>

Value at Risk pozwala ocenić z określonym prawdopodobieństwem, jak duża może być strata z inwestycji. Statystyka przemawia za tym, że skrajne scenariusze zdarzają się niezwykle rzadko. VaR 99% pozwala oszacować, jak duża strata nie zostanie przekroczona w 99 scenariuszach na 100. VaR informuje tylko o wartości granicznej, natomiast ES uzupełnia o informacje, jak duża jest wartość oczekiwana straty, pod warunkiem przekroczenia VaR. Czyli VaR mówi o tym, jaki najgorszy scenariusz spotka inwestora na 99%, a ES dodatkowo mówi mu, ile straci, jeśli już wydarzy się ten 1% najgorszych możliwości.

<br><br>
W projekcie przyjęto VAR 99%, dla okna czasowego 500 dni.

## 2.1. Metoda historyczna

<br>
Jest to najprostsza metoda, która na podstawie danych historycznych oblicza odpowiedni kwantyl rozkładu.

```{r}
Strata <- -euro$Log_stopa

metoda_historyczna <- function(dane)
{
  n = length(dane)
  VAR <- c()
  ES <- c()
  
  for(i in 501:n)
  {
    okno <- dane[(i-500):(i-1)]
    VAR[i] <- quantile(okno, 0.99)
    ES[i] <- mean(okno[okno > VAR[i]])
  }
  return(list(VAR=VAR, ES=ES))
}

euro$VAR_his <- metoda_historyczna(Strata)$VAR
euro$ES_his <- metoda_historyczna(Strata)$ES

w1 <- euro %>%
  ggplot(aes(x = Data)) +
  geom_line(aes(y = VAR_his, color = "VAR")) +
  geom_line(aes(y = ES_his, color = "ES")) +
  geom_line(aes(y = -Log_stopa), color = "grey") +
  labs(
    title = "Var i ES - metoda historyczna") +
scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "2 years"),
    date_labels = "%Y") +
  theme_bw()

w1
```

<br>
Dla metody historycznej charakterystyczny jest wykres schodkowy z dłuższymi fragmentam, kiedy VaR jest stały. Wynika to ze sposobu liczenia VaR w tej metodzie. Biorąc okno 500-dniowej historii wyznaczamy kwantyl 0.99. Przsuwając okno dalej o jedną obserwacje, jest mała szansa, że trafi się na tyle duża obserwacja, by zmienić VaR. Najczęściej natrafia się na przeciętnej wielkości straty, które nie wpływają na wielkość VaRu, dlatego jest on stały w podokresach. Podobnie działą ES.

<br>

## 2.2. Metoda z wagami

<br>
Metoda ta podchodzi do VaRu nieco inaczej. Zkłada, że najważniejsze jest to, co działo się na rynku niedawno, a czym starsze obserwacje, tym ich wpływ na wyznaczanie dzisiejszego VaRu powinien być mniejszy. Zgodnie z tym podejściem, nadaje się wagi prawdopodobieństwom. W oknie 500-dniowej historii już nie każda strata ma prawdopodobieństwo 1/500, ale prawdopodobieństwa są odpowiednio przeskalowane. VaR wyznacza się, kumulując prawdopodobieństwa do 0.99.

```{r}
metoda_wagi <- function(dane)
{
  j = 1:500
  probs <- c(0.995^(500-j) * (1 - 0.995) / (1 - 0.995^500)) # wagi dla prawdopodobieństw
  
  n = length(dane)
  VAR <- c()
  ES <- c()
  
  for(i in 501:n)
  {
    ramka <- data.frame(
      straty = dane[(i-500):(i-1)],
      p = probs) %>%
      arrange(straty, desc = FALSE) %>%  # sortuję rosnąco wg. stóp
      mutate(p_cum = cumsum(p)) # kumuluję prawdopodobieństwa
      
    # ramka tylko z tymi, które przekraczają kwantyl 0.99
    przekroczenie <- ramka %>%
      filter(p_cum > 0.99)
    VAR[i] <- przekroczenie$straty[1] # pierwsza strata, która przekracza skumulowane 0.99
    
    przekroczenie$p[1] <- 0.01 - sum(przekroczenie$p[-1]) # musi dopełniać do 0.01
    ES[i] <- sum(przekroczenie$straty * przekroczenie$p) / sum(przekroczenie$p) # średnia ważona
  }
  return(list(VAR=VAR, ES=ES))
}

euro$VAR_wagi <- metoda_wagi(Strata)$VAR
euro$ES_wagi <- metoda_wagi(Strata)$ES

w2 <- euro %>%
  ggplot(aes(x = Data)) +
  geom_line(aes(y = VAR_wagi, color = "VAR")) +
  geom_line(aes(y = ES_wagi, color = "ES")) +
  geom_line(aes(y = -Log_stopa), color = "grey") +
  labs(
    title = "Var i Es - metoda z wagami") +
  scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "2 years"),
    date_labels = "%Y") +
  theme_bw()

w2

```

<br>
Wykresy dla metody z wagami także dają charakterystyczny obraz. VaR jest przedziałami malejący, ale nie maleje gwałtownie, lecz schodkowo. Wynika to z tego, że przesuwając okno historii VaR może się nie zmieniać, jeśli nie natrafimy na wystarczająco duże straty, ale jednocześnie wagi ciągle maleją (starsze obserwacje tracą na znaczeniu). Podobnie ES czyli średnia strata jest malejąca, ale już nie schodkowo.

<br>

## 2.3. Metoda przeskalowania stóp

<br>
W tej metodzie prawdopodobieństwa zostają niezmienione, ale zmieniają się stopy zwrotu(strata). Metoda opiera się na obliczeniu zmienności znanymi metodami, np. EWMA lub GARCH. Następnie stopy zwrotu są aktualizowane tak, aby uwzględniały to, jaka zmienność jest w dniu, na który liczymy VaR. Robi się tak, ponieważ dla dużej zmienności stopy naturalnie będą duże, a dla małej zmienności będą niewielkie. 
<br>


```{r}
ewma <- function(data, lambda=0.94)
{
  n <- length(data)
  ewma <- numeric(n)
  ewma[1] <- var(data[1:25])
  
  for (i in 2:n)
  {
    ewma[i] <- lambda * ewma[i-1] + (1 - lambda) * data[i - 1]^2
  }
  ewma <- sqrt(ewma)
  
  return(ewma)
}

garch <- function(data)
{
  spec <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(1,1)),
                     mean.model = list(armaOrder = c(0,0), include.mean = F),
                     distribution.model = "norm")
  
  fit <- ugarchfit(spec, data = data)
  
  sigma_t <- sigma(fit)
  garch = as.vector(sigma_t)
  
  return(garch)
}

metoda_przeskalowane <- function(dane, metoda)
{
  n = length(dane)
  VAR <- c()
  ES <- c()
  
  if(metoda == "ewma") {zmiennosc <- ewma(dane)}
  else if(metoda == "garch") {zmiennosc <- garch(dane) }
  
  for(i in 501:n)
  {
    okno <- dane[(i-500):(i-1)]  
    
    # biorę odchylenie obecne (501 obserwacja względem okna które ma 1:500)
    zmiennosc501 <- zmiennosc[i]  
    # dzielę przez obecne odchylenie (działanie na wektorach)
    okno <- okno * zmiennosc501 / zmiennosc[(i-500):(i-1)] 
    
    VAR[i] <- quantile(okno, 0.99)
    ES[i] <- mean(okno[okno > VAR[i]])
  }
  return(list(VAR=VAR, ES=ES))
}
```

<br>

### 2.3.1. EWMA

<br>

```{r}
euro$VAR_ewma <- metoda_przeskalowane(Strata, metoda = "ewma")$VAR
euro$ES_ewma<- metoda_przeskalowane(Strata, metoda = "ewma")$ES

w3 <- euro %>%
  ggplot(aes(x = Data)) +
  geom_line(aes(y = VAR_ewma, color = "VAR")) +
  geom_line(aes(y = ES_ewma, color = "ES")) +
  geom_line(aes(y = -Log_stopa), color = "grey") +
  labs(
    title = "VaR i ES - EWMA") +
scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "2 years"),
    date_labels = "%Y") +
  theme_bw()

w3
```

<br>
VaR oraz ES dla przeskalowanych stóp z uwzględnieniem ich zmienności dają bardzo nieregularny i poszarpany wykres, który przypomina wyglądem wykres zmienności policzonej metodą EWMA. Widać zależność, że w tych okresach, kiedy zmienność jest duża, duży jest także VaR. Natomiast dla spokojniejszych okresów, czyli tam gdzie zmienność jest mała, a co za tym idzie zmniejsza się także ryzyko, VaR i ES także są małe.


<br>

### 2.3.2. GARCH

<br>

```{r}
euro$VAR_garch <- metoda_przeskalowane(Strata, metoda = "garch")$VAR
euro$ES_garch<- metoda_przeskalowane(Strata, metoda = "garch")$ES

w4 <- euro %>%
  ggplot(aes(x = Data)) +
  geom_line(aes(y = VAR_garch, color = "VAR")) +
  geom_line(aes(y = ES_garch, color = "ES")) +
  geom_line(aes(y = -Log_stopa), color = "grey") +
  labs(
    title = "VaR i ES - GARCH") +
scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "2 years"),
    date_labels = "%Y") +
  theme_bw()

w4
```

<br>
Wykres dla GARCH wygląda bardzo podobnie, jak dla EWMA. Na pierwszy rzut oka nie widać żadnych różnic. Zmienił się jednak zakres osi OY - dla GARCH VaR i ES są nieco niższe niż dla metody EWMA.

<br>

### 3.2.3. Porównanie

<br>
Na jednym wykresie zostanie przedstawione, jak wygląda zmienność liczona dwoma metodami.

```{r}
zmiennosc <- data.frame(Data = euro$Data)

zmiennosc_EWMA <- ewma(euro$Log_stopa)
zmiennosc <- cbind(zmiennosc, zmiennosc_EWMA)
zmiennosc_GARCH <- garch(euro$Log_stopa)
zmiennosc <- cbind(zmiennosc, zmiennosc_GARCH)

zmiennosc %>%
  ggplot() +
  geom_line(aes(x = Data, y = zmiennosc_EWMA, color = "EWMA")) +
  geom_line(aes(x = Data, y = zmiennosc_GARCH, color = "GARCH")) +
  labs(
    title = "Zmienność - metody EWMA i GARCH") +
scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "2 years"),
    date_labels = "%Y") +
  theme_bw()
```
<br>
Obie metody dają niemal identycznie rezultaty. Widać, że w pewnych miejscach EWMA jest niższe, ale po za tym kształt jest taki sam. Nie dziwi więc, że także i VaR i ES opierające się na tych dwóch różnych metodach dawałuy bliźniacze rezultaty.


<br>

## 2.4. Metoda Monte Carlo

<br>

Metoda ta jako jedyna w tym projekcie nie opiera się tylko na danych historycznych (z ewentualnymi modyfikacjami), ale przeprowadza symulacje. Na podstawie średniej i odchylenia danych historycznych, losowane są wartości z rozkładu normalnego i na ich podstawie wyznaczany jest VaR oraz ES. Losując wartości z zadanego rozkładu wiele razy dla każdego okna czasowego (np 10000 razy) można zasymulować, jak w rzeczywistości mogłyby zachowywać się straty. Odbywa się to oczywiście przy założeniu, że straty mają rozkład normalny, co zostało już wcześniej odrzucone, ale jednak na potrzeby tej metody wykorzystuje się to założenie.

```{r}
metoda_monte <- function(dane)
{
  n = length(dane)
  VAR <- c()
  ES <- c()
  
  for(i in 501:n)
  {
    okno <- dane[(i-500):(i-1)]
    symulacja <- rnorm(10000, mean(okno), sd(okno))
    VAR[i] <- quantile(symulacja, 0.99)
    ES[i] <- mean(symulacja[symulacja > VAR[i]])
  }
  return(list(VAR=VAR, ES=ES))
}


monte <- metoda_monte(Strata)
euro$VAR_monte <- monte$VAR
euro$ES_monte <- monte$ES


w5 <- euro %>%
  ggplot(aes(x = Data)) +
  geom_line(aes(y = VAR_monte, color = "VAR")) +
  geom_line(aes(y = ES_monte, color = "ES")) +
  geom_line(aes(y = -Log_stopa), color = "grey") +
  labs(
    title = "VaR i ES - symulacja Monte Carlo"
  ) +
scale_x_date(
    breaks = seq(as.Date("2007-01-01"), as.Date("2025-01-01"), by = "2 years"),
    date_labels = "%Y") +
  theme_bw()


w5

```

<br>
Ogólnym kształtem VaR i ES policzone dla metody symulacji Monte Carlo przypominają kształt z metody historycznej. Jest tak, ponieważ już po zastosowaniu symulacji, obliczany jest po prostu kwantyl 0.99, dokładnie jak w tamtej metodzie. Jednak tutaj symulacja nie daje VaRu okresowo, schodkowo stałego, ale jest bardzo gęsto nieregularnie poszarpany. Poszarpania te sa niewielkie, ale występują przez cały czas. Jest to spowodowane losowością, która została wprowadzona poprzez losowanie z rozkładu.

<br>

## 2.5. Porównanie wszystkich metod

<br>
Na jednym wykresie zostaną przedstawione wszystkie użyte metody (z powodu podobieństwa EWMA i GARCH na wykresie będzie tylko GARCH).

```{r}
w1 <- w1 + theme(axis.text.x = element_text(angle = 45, hjust = 1))
w2 <- w2 + theme(axis.text.x = element_text(angle = 45, hjust = 1))
w4 <- w4 + theme(axis.text.x = element_text(angle = 45, hjust = 1))
w5 <- w5 + theme(axis.text.x = element_text(angle = 45, hjust = 1))

grid.arrange(w1, w2, w4, w5, ncol = 2)
```

<br>
Każda z metod daje nieco inny obraz VaRu, charakterystyczny dla siebie. Mają jednak wspólną cehcę - tam, gdzie straty są duże tam VaR i Es są duże. Mają też podobną monotoniczność - maleją w tych samych okresach, ale sposób w jaki to robią jest inny.

<br>
VaR był największy od roku około 2009, kiedy to kurs euro osiągnął swoje minimum, a potem zaczął gwałtownie rosnąć. Drugi mocno zauważalny skok znajduje się około 2023 roku, kiedy kurs zmienił swoją monotoniczność i zaczął spadać po okresie w 2022 roku, kiedy za 1 euro trzeba było zapłacić niemal 5zł (efekt pandemii, wysokiej inflacji w Polsce i wojny w Ukrainie). Co ciekawe, jeśli chodzi o czas początku pandemii czyli około 2020 roku, VaR był wtedy dość niski.

<br><br>

# 3. Testy wsteczne

<br>
W celu stwierdzenia, czy VaR wyznaczony różnymi metodami jest poprawny oraz która z tych metod jest najbardziej efektywna, należy przeprowadzić testy wsteczne. Trzeba zliczyć ilość przekroczeń VaRU w każdym oknie czasowym. Dla VaRu 99% i okna o długości 500, liczba przekroczeń powinna wynosić około 5. Idealnie 5 wynosi zawsze dla metody historycznej. Dla innych metod ta liczba może się różnić. To, co znaczy "około 5" pozwalają stwierdzić tetsy takie jak test Kupca, test opraty na rozkładzie dwumianowym i test świateł. W tych testach sprawdzamy, czy liczba przekrocze mieści się w odpowiednim przedziale.

<br>

```{r}
zlicz_powyzej <- function(dane, VAR)
{
  ile_powyzej_var <- numeric(length(VAR))
  n = length(VAR)
  for(i in 501:n)
  {
    obecne_VAR = VAR[i]
    okno = dane[(i-500):(i-1)]
    ile_powyzej_var[i] <- length(okno[okno > obecne_VAR])
  }
  return(ile_powyzej_var)
}
```

<br>

## 3. Wykresy liczby przekroczeń

<br>

```{r}
kolumny_VAR <- names(euro)[grepl("VAR", names(euro))]
kolumny_VAR <- kolumny_VAR[-1] # bez VAR_hist

zliczone_VAR <- as.data.frame(sapply(kolumny_VAR, function(col_name) {
  zlicz_powyzej(Strata, euro[[col_name]])
}))

zliczone_VAR <- zliczone_VAR[501:nrow(zliczone_VAR),]

zliczone_VAR_long <- gather(zliczone_VAR, key = "rodzaj_VAR", value = "wartosc")
zliczone_VAR_long %>%
  ggplot() + 
  geom_histogram(aes(x = wartosc), binwidth = 1, fill = "#3366cc", color = "navy", alpha = 0.3) +
  facet_wrap(~ rodzaj_VAR, scales = "free") +  
  theme_bw()
```

<br>
Historgamy prezentują, ile razy VaR został przekroczony dla danej metody. Var liczony metodą z wagami oraz Monte Carlo dają dość podobne i zbliżone do siebie wyniki, natomiast różnią się one w stosunku do VARu liczonego z uwzględnieniem zmienności EWMA i GARCH.
<br>
Dla wag i Monte Carlo nie ma bardzo dużych przekroczeń VaRu. Maksymalnie VaR został przekroczony 16 razy, a najczęstsze były przekroczenia umiarkowane. Dla Monte Carlo znaleziono bardzo niewiele wyjątków poniżej 5 przekroczeń, natomiast dla wag znaleziono ich dużo, ale za to bardzo niewiele było przekroczeń powyżej 10. <br>
Jeśli chodzi o metody EWMA i GARCH, to w ich przypadku również zdecydowanie najwięcej przekroczeń to przekroczenia małe i średnie - od 0 przekroczeń do mniej więcej 10. Jednak przekroczenia nie kończą się na liczbie 16, jak było to w przypadku poprzednich metod, ale są przypadki nawet ponad 60 przekrocze VaRu. W 500 dniowej historii oznacza to, że aż 12% stóp zwrotu były wyższe niż wskazywałby na to VaR. Przekroczenia powyżej 40 zdarzają się już nie więcej niż 25 razy, ale wciąż jest to za często.

<br>

## 3.1. Test Kupca

<br>

Test ten sprawdza, czy liczba wyjątków mieści się w odpowiednim przedziale. W przypadku 500-dniowej historii i VaRu 99%, powinny mieścić się one w granicach od 1 do 10 (według statystyki o rozkładzie Chi2). Liczba wyjątków wpadająca do tego przedziału jest dobrym wynikiem. Natomiast 0 przekroczeń lub 11 i więcej przekroczeń to już wyniki alarmujące.

<br>

```{r}

test_kupca <- function(dane_zliczone)
{
  n = length(dane_zliczone) 
  poprawne <- ifelse(dane_zliczone >= 1 & dane_zliczone <= 10, 1, 0) 
  powyzej <- ifelse(dane_zliczone > 10, 1, 0)
  ponizej <- ifelse(dane_zliczone == 0, 1, 0)
  
  
  suma_poprawne <- sum(poprawne, na.rm = TRUE) 
  suma_powyzej <- sum(powyzej, na.rm = TRUE)
  suma_ponizej <- sum(ponizej, na.rm = TRUE)
  
  wynik <- c(suma_poprawne,
             n - suma_poprawne,
             paste0(round(suma_poprawne / n * 100, 2), "%"),
             suma_powyzej,
             suma_ponizej)
  
  return(wynik) 
}

wyniki_kupiec <- as.data.frame(sapply(kolumny_VAR, function(col_name) {
  test_kupca(zliczone_VAR[[col_name]])
}))

rownames(wyniki_kupiec) <- c("liczba poprawnych",
                             "liczba blednych",
                             "procent poprawnych",
                             "powyzej VAR",
                             "ponizej VAR")

kable(wyniki_kupiec)
```

<br>
W tabeli przedstawiona jest liczba "poprawnych" VaRów, czyli liczba wyjątków mieszczących się w przedziale od 1 do 10 oraz liczba niepoprawnych spoza tego przedziału. Dodatkowo obliczono odsetek poprawnych, a także liczbę tych wyjątków, które są poniżej przedziału (czyli wynoszą 0) i liczbę tych, które są powyżej przedziału. <br>

Najlepiej według tego testu radzi sobie metoda z wagami, w której aż 94% wszystkich wyjątków mieści się w dobrym zakresie. Dla tej metody nigdy nie było też sytuacji, że liczba przekroczeń VaRu wyniosła 0. Metoda Monte Carlo radzi sobie nieco gorzej (84% wyjątków w poprawnym przedziale). Również tutaj nie ma sytuacji, że byłoby 0 wyjątków. Natomiast VaR z przeskalowanymi stopami zarówno metodami EWMA i GARCH radzą sobie według tego testu bardzo źle. Ponad połowa wyjątków nie mieści się w zalecanym przedziale. Dodatkowo, najczęściej są to wyjątki przekraczającę przedział od góry (a około 160 i 130 to te, kiedy liczba wyjątków wynosiła 0). Wyniki mogą być takie, ponieważ okno, w którym wyznaczamy VaR oraz wyznaczamy liczbę wyjątków (500 dni) jest bardzo długie. To prawie 2 lata, w których mogły dziać się bardzo różne historie w danych. Ciężko porównywać VaR z dzisiaj, do tego, jaki był niemal dwa lata temu. VaR dla EWMA i GARCH aktualizują stopy według współczesnej zmienności, co pomaga policzyć VaR **na dziś**, ale badamy go 500 dni wstecz, kiedy zmienność mogła być zupełnie inna i zapewne wahała się w tym okresie kilka razy.

<br>

## 3.2. Test świateł

<br>

W teście świateł, podobnie jak w teście Kupca, sprawdza się, czy liczba wyjątków wpada do odpowiedniego przedziału. Tym razem klasyfikujemy wyjątki do trzech przedziałów - zielony, czyli kiedy wszystko jest dobrze, żółty - oznacza, że mogą być pewne nieprawidłowosci i należy bardziej kontrolować sytuację oraz czerwony, czyli alarmujący, że VaR może być liczony źle.

<br>

```{r}
# Obliczenie granic testu świateł dla 500-dniowej historii
swiatla_granice <- data.frame(
  wyjatek = 0:20,
  q_skumulowane = round(sapply(0:20, function(x) pbinom(x, 500, 0.01)),4)
) %>%
  mutate(swiatlo = ifelse(q_skumulowane < 0.95, "green", 
                          ifelse(q_skumulowane >= 0.95 & q_skumulowane < 0.9999, "yellow", "red")))

kable(swiatla_granice)
```

<br> Dla przedziału 0-8 kolor to zielony, przedział 9-14 to kolor żółty, natomiast powyżej 14 wyjątków to już kolor czerwony. Takie wyniki otrzymuje się kumulując dystrybuantę rozkładu dwumianowego. Poniżej 0.95 kolor jest zielony, a powyżej 0.9999 czerwony.

<br>

```{r}
test_lights <- function(dane_zliczone)
{

  test_swiatel <- c()
  n = length(dane_zliczone)
  for(i in 1:n)
  {
    if(dane_zliczone[i] <= 9) { test_swiatel[i] <- "zielone" }
    else if(dane_zliczone[i] <= 15) { test_swiatel[i] <- "zolte" }
    else { test_swiatel[i] <- "czerwone"}
  }
  
  test_swiatel <- factor(test_swiatel, levels = c("zielone", "zolte", "czerwone"))
  wynik <- table(test_swiatel)
  
  return(as.vector(wynik))
}

wyniki_lights <- as.data.frame(sapply(kolumny_VAR, function(col_name) {
  test_lights(zliczone_VAR[[col_name]])
}))

rownames(wyniki_lights) <- c("green", "yellow", "red")

kable(wyniki_lights)
```

<br>
W przypadku metody z wagami nie ma przypadku czerwonego koloru, natomiast Metoda Monte Carlo ma zaledwie kilka takich przypadków. Dla obu tych metod liczba wyjątków zdecydowanie najczęściej należy do przedziału zielonego, a tylko niecałe 14% (w przypadku wag) i 20% (w przypadku Monte Carlo) należą do przedziału z kolorem żółtym. Można więc wysnuć wnioski, że obie te metody radzą sobie naprawdę bardzo dobrze z przewidywaniem VaRu dla kursu EUR-PLN. W przypadku dwóch pozostałych metod jest bardzo dużo wyjątków w strefie czerwonej. Jest ich prawie tyle, co tych w strefie zielonej. Po zsumowaniu wszystkich wyjątków ze strefy czerwonej oraz żółtej, jest ich więcej niż bezpiecznych przypadków ze strefy zielonej.

<br>

## 3.3. Test niezależności - Test Christoffersona

<br>
Oprócz badania, czy liczba wyjątków mieści się w odpowiednim przedziale, równie ważne jest zbadanie, czy wyjątki te są niezależne. Kiedy wyjątków zdarza się więcej niż powinno, ale są niezależne, to jeszcze nie musi wskazywać to na złą sytuację. Dużo gorzej jest wtedy, gdy liczba przekroczeń nie jest niezależna. Oznacza to, że wyjątki pojawiają się w krótkim odstępie czasu i bank czy firma, która monitoruje VaR, musi w krótkim czasie mieć przygotowane bardzo dużo "zapasów" na tę sytuację.
<br>
Niezależność przekroczeń VaRu w czasie bada **test Christoffersona.** <br><br>

**H0:** Przekroczenia VaR są niezależne w czasie. <br>
**H1:** Przekroczenia VaR nie są niezależne w czasie. <br><br>



```{r}
test_christoffersona <-function(strata, VAR)
{
  
  wyniki <-c()
  
  for(i in 501:(length(strata)))
  {
    wektor <- strata[(i-500):(i-1)]
    var <- VAR[i]
    przekroczenia <- wektor > var
    
    u00 <- sum(sapply(1:(length(przekroczenia)-1), function(x) (ifelse(przekroczenia[x]==0 & przekroczenia[x+1] == 0, 1, 0))))
    u01 <- sum(sapply(1:(length(przekroczenia)-1), function(x) (ifelse(przekroczenia[x]==0 & przekroczenia[x+1] == 1, 1, 0))))
    u10 <- sum(sapply(1:(length(przekroczenia)-1), function(x) (ifelse(przekroczenia[x]==1 & przekroczenia[x+1] == 0, 1, 0))))
    u11 <- sum(sapply(1:(length(przekroczenia)-1), function(x) (ifelse(przekroczenia[x]==1 & przekroczenia[x+1] == 1, 1, 0))))
    
    pi <-  (u01 + u11) / (u00 + u01 + u10 + u11)
    pi01 <- u01 / (u00 + u01)
    pi11 <- u11 /( u10 + u11)
    
    a <- log((1-pi)^(u00+u10)*(pi)^(u01+u11))
    b <- log((1-pi01)^(u00) * (pi01)^(u01) * (1-pi11)^(u10) * pi11^u11)
    statystyka <- -2 * a + 2 * b
    
    wyniki <- c(wyniki, statystyka)
  }
  return (c(rep(NA,500),wyniki))
}


christofferson_wyniki <- function(statystyki)
{
  p_value <- 1 - pchisq(statystyki, df = 1)
  zalezne <- sum(p_value < 0.05, na.rm = TRUE)
  niezalezne <- sum(p_value >= 0.05, na.rm = TRUE)
  
  return(c(zalezne, niezalezne))
}


ramka_christofferson <- data.frame(
  historyczna = christofferson_wyniki(test_christoffersona(Strata, euro$VAR_his)),
  wagi = christofferson_wyniki(test_christoffersona(Strata, euro$VAR_wagi)),
  ewma = christofferson_wyniki(test_christoffersona(Strata, euro$VAR_ewma)),
  garch = christofferson_wyniki(test_christoffersona(Strata, euro$VAR_garch)),
  monte = christofferson_wyniki(test_christoffersona(Strata, euro$VAR_monte))
)

rownames(ramka_christofferson) <- c("zależne", "niezależne")
kable(ramka_christofferson)
```
<br>
Test Christoffersona pozwolił zliczyć, dla ilu 500-dniowych okien czasowych wsytępujące w nich przekroczenia są niezależne. Wyniki są zadowalające. W zdecydowanej większości przekroczenia występują od siebie niezależnie w czasie. Także dla EWMA i GARCH, dla których było najwięcej alarmujących przekroczeń, wychodzi na to, że są one od siebie najczęściej niezależne. Najgorzej pod tym względem poradziła sobie metoda historyczna, gdzie dla około 1/3 okien te przekroczenia jednak są od siebie zależne. Metoda Monte Carlo dała najlepsze rezultaty, a zaraz za nią metoda z wagami. Nieco słabiej wypadają GARCH i EWMA, ale dają wciąż zadowalające wyniki.

<br><br>

# 4. Podsumowanie

<br>
Projekt umożliwił zastosowanie i ocenę pięciu różnych metod liczenia VaR oraz trzech testów wstecznych (dwóch dystrybucyjnych oraz jednego niezależności). Na tej podstawie można stwierdzić, że dla danych dotyczących kursu EUR-PLN najlepiej poradziły sobie metoda historyczna z wagami oraz metoda symulacji Monte Carlo. Obie te metody miały nie tylko najmniejszą liczbę wyjątków, które przekraczały odpowiednie przedziały, ale przekroczenia jeśli już się zdarzały, to były niezależne. Metoda historyczna natomiast, pomimo jej idealnego dopasowania (zawsze liczba wyjątków powyżej VaR wynosi 5), jednak często posiada wyjątki zależne od siebie. Metody EWMA i GARCH słabo radziły sobie w tak długiej historii, ponieważ na bieżąco aktualizują one zmienność, która nie jest adekwatna 500 dni wstecz. Dodatkowo, miały większą ilość zależnych wyjątków niż metoda z wagami czy Monte Carlo.









